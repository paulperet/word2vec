# Word2Vec using Skip-Gram and Noise Contrastive Estimation

This program can be used to train a word embedding model from any text file (with BERT tokenizer). 
This is an implementation of: Mikolov, Tomas & Sutskever, Ilya & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). 
Distributed Representations of Words and Phrases and their Compositionality. Advances in Neural Information Processing Systems. 26. 

# Installation
Project used Python 3.12.9 but other versions may be suitable

Clone the project:
```bash
git clone https://github.com/paulperet/word2vec
```

Go to the project's folder
```bash
cd word2vec
```

Create a virtual environment
```bash
python -m venv .venv
```

Activate the environment
```bash
source .venv/bin/activate
```

Install the dependancies
```bash
pip install -r requirements
```
